---
title: "Rank-normalized split-Rhat and relative efficiency estimates"
author: "Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, Paul BÃ¼rkner"
date: "First version 2017-05-23. Last modified `r format(Sys.Date())`."
encoding: "UTF-8"
output:
  html_document:
    fig_caption: yes
    toc: TRUE
    toc_depth: 2
    number_sections: TRUE
    toc_float:
      smooth_scroll: FALSE
bibliography: rhat_neff.bib
csl: harvard-cite-them-right.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  comment = NA, 
  cache = FALSE,
  fig.width = 10
)
options(width = 90)
```

# Introduction

The Split-$\widehat{R}$ statistic and the effective sample size $S_{\rm eff}$
(previously called $N_{\rm eff}$ or $n_{\rm eff}$) are routinely used to monitor
the convergence of iterative simulations, which are omnipresent in Bayesian
statistics in the form of Markov-Chain Monto-Carlo samples. The original
$\widehat{R}$ statistic [@Gelman+Rubin:1992; @Brooks+Gelman:1998] and
*split*-$\widehat{R}$ [@BDA3] are both based on the ratio of between and
within-chain marginal variances of the simulations, while the latter is computed
from split chains (hence the name).

$\widehat{R}$, *split*-$\widehat{R}$, and $S_{\rm eff}$ are well defined only if
the marginal distributions have finite mean and variance. Even if that's the
case, their estimates are less stable for distributions with long tails. To
alleviate these problems, we define *split*-$\widehat{R}$ and $S_{\rm eff}$
using rank normalized values, empirical cumulative density functions, and small
posterior intervals.

The code for the new *split*-$\widehat{R}$ and $S_{\rm eff}$ versions and for
the corresponding diagnostic plots can be found in [monitornew.R](monitornew.R)
and [monitorplot.R](monitorplot.R), respectively. An extended case study with
more details and experiments is available at
[rhat_reff_extra](rhat_reff_extra.html).

# Review of *split*-$\widehat{R}$ and effective sample size

In this section, we will review the split-$\widehat{R}$ and 
effective sample size estimates as implemented in Stan 2.18 [@Stan.2.18].
These implementations represent the current de facto standard of convergence
diagnostics for iterative simulations.

## *Split*-$\widehat{R}$ {#SplitRhat}

Below, we present the computation of *Split*-$\widehat{R}$ following
@BDA3, but using the notation style of @StanBook. Our general recommendation
is to always run several chains. $N$ is the number of draws per chain, $M$ is
the number of chains, and $S=MN$ is the total number of draws from all
chains. For each scalar summary of interest $\theta$, we
compute $B$ and $W$, the between- and within-chain variances:

<!--
$$
B = \frac{N}{M-1}\sum_{m=1}^{M}(\overline{\theta}_{.m}-\overline{\theta}_{..})^2,
\;\mbox{ where }\;\;\overline{\theta}_{.m}=\frac{1}{N}\sum_{n=1}^nN \theta_{nm},\;\;
\;\;\overline{\theta}_{..} = \frac{1}{M}\sum_{m=1}^M\overline{\theta}_{.m}\\
W =
 \frac{1}{M}\sum_{m=1}^{M}s_j^2, \;\mbox{ where }\;\; s_m^2=\frac{1}{N-1}
\sum_{n=1}^N (\theta_{nm}-\overline{\theta}_{.m})^2.
$$
-->

$$
B = \frac{N}{M-1}\sum_{m=1}^{M}(\overline{\theta}^{(.m)}-\overline{\theta}^{(..)})^2,
\;\mbox{ where }\;\;\overline{\theta}^{(.m)}=\frac{1}{N}\sum_{n=1}^N \theta^{(nm)},\;\;
\;\;\overline{\theta}^{(..)} = \frac{1}{M}\sum_{m=1}^M\overline{\theta}^{(.m)}
\\
W = \frac{1}{M}\sum_{m=1}^{M}s_j^2, \;\mbox{ where }\;\; s_m^2=\frac{1}{N-1}
\sum_{n=1}^N (\theta^{(nm)}-\overline{\theta}^{(.m)})^2.
$$

The between-chain variance, $B$, also contains the factor $N$ because it is
based on the variance of the within-chain means, $\overline{\theta}^{(.m)}$,
each of which is an average of $N$ values $\theta^{(nm)}$.

We can estimate $\mbox{var}(\theta \mid y)$, the marginal posterior variance
of the estimand, by a weighted average of $W$ and $B$, namely
$$
\widehat{\mbox{var}}^+(\theta \mid y)=\frac{N-1}{N}W + \frac{1}{N}B.
$$
This quantity *overestimates* the marginal posterior variance assuming the
starting distribution of the simulations is appropriately overdispersed compared
to the target distribution, but is *unbiased* under stationarity (that is, if
the starting distribution equals the target distribution), or in the limit
$N\rightarrow\infty$. To have an overdispersed starting distribution,
independent Markov chains should be initialized with diffuse starting values for
the parameters.

Meanwhile, for any finite $N$, the within-chain variance
$W$ should *underestimate* $\mbox{var}(\theta \mid y)$ because the 
individual chains haven't had the time to explore all of the target 
distribution and, as a result, will have less variability. 
In the limit as $N\rightarrow\infty$, the expectation of $W$ also
approaches $\mbox{var}(\theta \mid y)$.

We monitor convergence of the iterative simulations to the target
distribution by estimating the factor by which the scale of the 
current distribution for $\theta$ might be reduced if the simulations 
were continued in the limit $N\rightarrow\infty$. This potential 
scale reduction is estimated as
$$
\widehat{R}=
\sqrt{\frac{\widehat{\mbox{var}}^+(\theta \mid y)}{W}},
$$
which declines to 1 as $N\rightarrow\infty$. We call this *split*-$\widehat{R}$
because we are applying it to chains that have been split in half so that $M$ is
twice the number of actual chains. Without splitting, $\widehat{R}$ would get
fooled by non-stationary chains.

We note that *split*-$\widehat{R}$ is also well defined for sequences
that are not Markov-chains. However, for simplicity, we always refer
to 'chains' instead of more generally to 'sequences' as the former is
our primary use case for $\widehat{R}$-like measures.

## Effective sample size $S_{\rm eff}$ {#Seff}

If the $N$ simulation draws within each chain were truly
independent, the between-chain variance $B$ would be an unbiased
estimate of the posterior variance, $\mbox{var}(\theta \mid y)$, and we would have
a total of $S = M \times N$ independent simulations from the $M$
chains. In general, however, the simulations of $\theta$ within each
chain will be autocorrelated, and thus $B$ will be larger than 
$\mbox{var}(\theta \mid y)$, in expectation.

A nice introductory reference for analyzing MCMC results in general
and effective sample size in particular is provided by @Geyer:2011
[see also @Geyer:1992].  The particular calculations used by Stan
[@Stan.2.18] follow those for split-$\hat{R}$, which involve both
between-chain (mean) and within-chain calculations
(autocorrelation). They were introduced in the Stan manual
[@StanManual.2.18.0] and explained in more detail in @BDA3.

One way to define effective sample size for correlated simulation
draws is to consider the statistical efficiency of the average of the
simulations $\bar{\theta}^{(..)}$ as an estimate of the posterior mean
$\mbox{E}(\theta \mid y)$. This can be a reasonable baseline even though it is
not the only possible summary and might be inappropriate, for example,
if there is particular interest in accurate representation of
low-probability events in the tails of the distribution.

The effective sample size of a chain is defined in terms of the
autocorrelations within the chain at different lags. The
autocorrelation $\rho_t$ at lag $t \geq 0$ for a chain with joint
probability function $p(\theta)$ with mean $\mu$ and variance
$\sigma^2$ is defined to be
$$
\rho_t 
=
\frac{1}{\sigma^2} \, \int_{\Theta} (\theta^{(n)} - \mu)
(\theta^{(n+t)} - \mu) \, p(\theta) \, d\theta.
$$
This is just the correlation between the two chains offset by $t$
positions.  Because we know $\theta^{(n)}$ and $\theta^{(n+t)}$ have
the same marginal distribution in an MCMC setting, multiplying the
two difference terms and reducing yields
$$
\rho_t
=
\frac{1}{\sigma^2} \, \int_{\Theta} \theta^{(n)} \, \theta^{(n+t)} \, p(\theta) \, d\theta.
$$

The effective sample size of one chain generated by a process with
autocorrelations $\rho_t$ is defined by
$$
N_{\rm eff}
\ = \
\frac{N}{\sum_{t = -\infty}^{\infty} \rho_t}
\ = \
\frac{N}{1 + 2 \sum_{t = 1}^{\infty} \rho_t}.
$$

<!--
Paul: Here, you still use Neff, but elsewhere you consistently use Seff.
Is this intentional?
-->

Effective sample size $N_{\rm eff}$ can be larger than $N$ in case of antithetic
Markov chains, which have negative autocorrelations on odd lags. The Dynamic
Hamiltonian Monte-Carlo algorithms used in Stan [@Hoffman+Gelman:2014] can
produce $N_{\rm eff}>N$ for parameters with a close to Gaussian posterior
and low dependency on other parameters.

### Estimation of the Effective Sample Size

In practice, the probability function in question cannot be tractably
integrated and thus neither autocorrelation nor the effective sample size
can be calculated. Instead, these quantities must be estimated
from the samples themselves. The rest of this section describes an
autocorrelation and split-$\hat{R}$ based effective sample
size estimator, based on multiple split chains. For simplicity, 
each chain will be assumed to be of the same length $N$.

Stan carries out the autocorrelation computations for all lags
simultaneously using Eigen's fast Fourier transform (FFT) package with
appropriate padding; see @Geyer:2011 for more details on using
FFT for autocorrelation calculations.
The autocorrelation estimates $\hat{\rho}_{t,m}$ at lag $t$ from
multiple chains $m \in (1,\ldots,M)$ are combined with the within-chain
variance estimate $W$ and the multi-chain variance estimate
$\widehat{\mbox{var}}^{+}$ introduced in the previous section to
compute the combined autocorrelation at lag $t$ as
$$
\hat{\rho}_t
= 1 - \frac{\displaystyle W - \textstyle \frac{1}{M}\sum_{m=1}^M 
\hat{\rho}_{t,j}}{\widehat{\mbox{var}}^{+}}. \label{rhohat}
$$
If the chains have not converged, the variance estimator
$\widehat{\mbox{var}}^{+}$ will overestimate the true marginal variance 
which leads to an overestimation of the autocorrelation and an 
underestimation of the effective sample size.

Because of noise in the correlation estimates $\hat{\rho}_t$ increases as
$t$ increases, typically the truncated sum of $\hat{\rho}_t$ is used.
Negative autocorrelations can happen only on odd lags and by summing
over pairs starting from lag $t=0$, the paired autocorrelation is
guaranteed to be positive, monotone and convex modulo estimator noise
[@Geyer:1992; @Geyer:2011].  Stan 2.18 uses Geyer's initial monotone
sequence criterion. The effective sample size of combined chains is defined as
$$
S_{\rm eff} = \frac{N \, M}{\hat{\tau}},
$$
where
$$
\hat{\tau} = 1 + 2 \sum_{t=1}^{2k+1} \hat{\rho}_t = 
-1 + 2 \sum_{t'=0}^{k} \hat{P}_{t'},
$$
and $\hat{P}_{t'}=\hat{\rho}_{2t'}+\hat{\rho}_{2t'+1}$. The initial
positive sequence estimator is obtained by choosing the largest $k$
such that $\hat{P}_{t'}>0$ for all $t' = 1,\ldots,k$. The initial monotone
sequence estimator is obtained by further reducing $\hat{P}_{t'}$ to the minimum
of the preceding values so that the estimated sequence becomes monotone.

<!--
Paul: Does \hat{\tau} have a specific name we could use? 
Is this monotone sequence finally used to compute \hat{\tau}? 
This should be made explicit.
-->

The effective sample size $S_{\rm eff}$ described here is different from similar
formulas in the literature in that we use multiple chains and between-chain
variance in the computation, which typically gives us more conservative claims
(lower values of $S_{\rm eff}$) compared to single chain estimates, especially
when mixing of the chains is poor. If the chains are not mixing at all (e.g.,
the posterior is multimodal and the chains are stuck in different modes), then
our $S_{\rm eff}$ is close to the number of chains.

Before version 2.18, Stan used a slightly incorrect initial sequence which
implied that $S_{\rm eff}$ was capped at $S$ and thus the effective sample size
was underestimated for some models. As antithetic behavior (i.e., $S_{\rm eff} >
S$) is not that common or the effect is small, and capping at $S$ can be
considered to be pessimistic, this had negligible effect on any inference.
However, it may have led to an underestimation of Stan's efficiency when
compared to other packages performing MCMC sampling.

# Rank normalized *split*-$\widehat{R}$ and relative efficiency estimates

As *split*-$\widehat{R}$, and $S_{\rm eff}$ are well defined only if
the marginal posteriors have finite mean and variance, we next
describe variants which are well defined for all distributions and
more robust for long tailed distributions.

## Rank normalization

1. Rank: Replace each value $\theta^{(nm)}$ by its rank $r^{(nm)}$. Average
rank for ties are used to conserve the number of unique values of discrete
quantities. Ranks are computed jointly for all draws from all chains.
2. Normalize: Normalize ranks by inverse normal transformation $z^{(nm)} =
\phi^{-1}((r^{(nm)}-1/2)/S)$. 

For continuous variables and $S \rightarrow \infty$, the rank
normalized values are normally distributed and rank normalization
performs non-parametric transformation to normal distribution. Using
normalized ranks instead of ranks directly, has the benefit that the
behavior of $\widehat{R}$ and $S_{\rm eff}$ do not change for normally
distributed $\theta$.

<!--
Paul: What exactly is implied by "rank normalization
performs non-parametric transformation to normal distribution"?
What's the importance of "non-parametric" here?
I think the purpose of this statement needs to be better explained
-->

## Rank normalized *Split*-$\widehat{R}$

Rank normalized *Split*-$\widehat{R}$ is computed using the equations in
[Section *Split*-$\widehat{R}$](#SplitRhat) by replacing the original parameter
values $\theta^{(nm)}$ with their corresponding rank normalized values
$z^{(nm)}$.

## Rank normalized folded *split*-$\widehat{R}$

Both original and rank-normalized *Split*-$\widehat{R}$ can be fooled if
chains have different scales but the same location. To alleviate this problem,
we propose to compute rank normalized *folded-split*-$\widehat{R}$
using *folded* split chains by rank normalizing absolute deviations from
median
$$
  {\rm abs}(\theta^{(nm)}-{\rm median}(\theta)).
$$

<!--
Notation for median?
Paul: I like the current notation as it is super explicit
-->

To obtain a single conservative $\widehat{R}$ estimate, we propose to
report the maximum of *rank-normalized-split*-$\widehat{R}$ and
*rank-normalized-folded-split*-$\widehat{R}$ for each parameter.

## Relative efficiency using rank normalized values

In addition to using rank-normalized values for convergence
diagnostics via $\widehat{R}$, we can also compute the corresponding
effective sample size.  This estimate will be well defined even if the
original distribution does not have finite mean and variance. It is
not directly applicable to estimate the Monte Carlo error of the mean
of the original values, but it will provide a bijective transformation-invariant estimate of the mixing efficiency of chains. For simplicity
we propose to report relative efficiency values
$$
R_{\rm eff}=\textit{rank-normalized-split-}S_{\rm eff} / S,
$$
where $\textit{split-}S_{\rm eff}$ is computed using equations in
[Section Effective sample size](#Seff) by replacing parameter values
$\theta^{(nm)}$ with rank normalized values $z^{(nm)}$. For parameters with
a close to normal distribution, the difference to using the original
values is small. However, for parameters with a distribution far from
normal, rank normalization can be seen as a near optimal non-parametric
transformation.

<!--
Paul: Do we have a citation for the last statment?
-->

The relative efficiency estimate using rank
normalized values is a useful measure for relative efficiency of
estimating the bulk (mean and quantiles near the median) of the
distribution, and as shorthand term we use term *bulk relative
efficiency* (*bulk*-$R_{\rm eff}$).

We propose to compute the relative efficiency also using *folded*
split chains by rank normalizing absolute deviations from median (see above),
which is a useful measure for the relative efficiency of estimating the 
distributions' tail. As a shorthand, we use the term *tail relative
efficiency* (*tail*-$R_{\rm eff}$).

## Relative efficiency of the cumulative distribution function

The bulk and tail relative efficiency measures introduced above are useful as
overall efficiency measures. Next, we introduce relative efficiency estimates of
the cumulative distribution function (CDF), and later we use that to introduce
relative efficiency diagnostics of quantiles and local small probability
intervals.

Quantiles and posterior intervals derived on their basis are often reported
quantities which are easy to estimate from posterior draws.  Estimating the
relative efficiency of such quantiles thus has a high practical relevance in
particular as we observe the relative efficiency for tail quantiles to often be
lower than for the mean or median. The $\alpha$-quantile is defined as the parameter value
$\theta_\alpha$ for which $p(\theta \leq \theta_\alpha) = \alpha$. An estimate
$\hat{\theta}_\alpha$ of $\theta_\alpha$ can thus be obtained by finding the
$\alpha$-quantile of the empirical CDF (ECDF) of the posterior draws
$\theta^{(s)}$. However, quantiles cannot be written as an expectation, and thus
the above equations for $\widehat{R}$ and $S_{\rm eff}$ are not directly
applicable. Thus, we first focus on the relative efficiency of the cumulative
probability $p(\theta \leq \theta_\alpha)$ for different values of $\theta_\alpha$.

For any $\theta_\alpha$, the ECDF gives an estimate of the cumulative 
probability
$$
p(\theta \leq \theta_\alpha) \approx \bar{I}_\alpha = \frac{1}{S}\sum_{s=1}^S
I(\theta^{(s)} \leq\theta_\alpha),
$$
where $I()$ is the indicator function. The indicator function transforms
simulation draws to 0's and 1's, and thus the subsequent computations are
bijectively invariant. Efficiency estimates of the ECDF at any $\theta_\alpha$ can
now be obtained by applying rank-normalizing and subsequent computations
directly on the indictor function's results.

## Relative efficiency of quantiles {#quantile_R_eff}

Assuming that we know the CDF to be a certain contionous function $F$ which is
smooth near an $\alpha$-quantile of interest, we could use the delta method to
compute a variance estimate for $F^{-1}(\bar{I}_\alpha)$. Although we
don't usually know $F$, the delta method approach reveals that the variance of
$\bar{I}_\alpha$ for some $\theta_\alpha$ is scaled by the (usually unknown)
density $f(\theta_\alpha)$, but the relative efficiency depends only on the relative
efficiency of $\bar{I}_\alpha$. Thus, we can use the relative efficiency of
the ECDF computed via the indicator function 
$I(\theta^{(s)} \leq \theta_\alpha)$ also for the corresponding quantile 
estimates.

## Relative efficiency of median and MAD

Since the marginal posterior distributions might not have finite mean
and variance, by default RStan [@RStan.2.17] and RStanARM
[@RStanARM.2.17] report median and median absolute deviation (MAD)
instead of mean and standard error (SE). Median and MAD are well
defined even when the marginal distribution does not have finite mean
and variance. Since the median is just 50%-quantile, we can estimate its 
relative efficiency as for any other quantile.  

We can also compute the relative efficiency for the
median absolute deviation (MAD), by computing the relative efficiency
for the median of absolute deviations from the median of all draws. The
absolute deviations from the median of all draws are same as
previously defined for folded samples
$$
{\rm abs}(\theta^{(nm)}-{\rm median}(\theta)).
$$
We see that the relative efficiency of MAD is obtained by using the
same approach as for the median (and other quantiles) but with the folded
values also used in *rank-normalized-folded-split*-$S_{\rm eff}$.

## Monte Carlo error estimates for quantiles

Previously, Stan has reported Monte Carlo standard error estimates for the mean
of a quantity. This is valid only if the corresponding distribution has finite
mean and variance; and even if valid, it may be easier and more robust to focus
on the median and other quantiles, instead.

Median, MAD and quantiles are well defined even when the distribution does not
have finite mean and variance, and they are asymptotically normal for continuous
distributions which non-zero in the relevant neighborhood. As the delta method
for computing the variance would require explicit knowledge of the normalized 
posterior density, we propose the following alternative approach:

1. Compute quantiles of the
${\rm Beta}(R_{\rm eff} \bar{I}_\alpha+1, R_{\rm eff}(1-\bar{I}_\alpha)+1)$
distribution. Including $R_{\rm eff}$ 
takes into account the relative efficiency of the posterior draws.
2. Find indices in $\{1,\ldots,S\}$ closest to the ranks of these quantiles. 
For example, for quantile $Q$, find $s = {\rm round(Q S)}$.
3. Use the corresponding $\theta^{(s)}$ from the list of sorted
posterior draws as quantiles from the error distribution. These quantiles can 
be used to approximate the Monte Carlo standard error.

<!--
Paul: These steps are still rather vague. We should try to make them more explicit.
Step 1 needs to state which quantiles are extracted. I see
that from the code but I can only guess the justification
Step 3 needs more explanation how these quantiles are used. Again,
I can infer that from the code, but I am not entirely sure about
the justification.
-->

## Relative efficiency of small interval probability estimates {#small_interval_R_eff}

We can get more local relative efficiency estimates by considering
small probability intervals. We propose to compute the relative
efficiencies for
$$
\bar{I}_{\alpha,\delta} = p(\hat{Q}_\alpha < \theta \leq \hat{Q}_{\alpha+\delta}),
$$
where $\hat{Q}_\alpha$ is an empirical $\alpha$-quantile, $\delta=1/k$
is the length of the interval with some positive integer $k$, and
$\alpha \in (0,\delta,\ldots,1-\delta)$ changes in steps of $\delta$.
Each interval has $S/k$ draws, and the efficiency measures
autocorrelation of an indicator function which is $1$ when the values
are inside the specific interval and $0$ otherwise. This gives us a
local efficiency measure which does not depend on the shape of the
distribution.

## Rank plots

In addition to using rank-normalized values to compute
*split*-$\widehat{R}$, we propose to use rank plots for each chain
instead of trace plots. Rank plots are nothing else than histograms of the 
ranked posterior samples (ranked over all chains) plotted separately
for each chain. If rank plots of all chains look similar, this indicates
good mixing of the chains. As compared to trace plots, rank plots don't tend
to squeeze to a mess in case of long chains.

# Examples

In this section, we will go through several examples to demonstrate the
usefulness of our proposed methods as well as the associated workflow in
determining convergence. First, we load all the necessary R packages and
additional functions.

```{r, comment=NA, message=FALSE, warning=FALSE, results='hide', cache=FALSE}
library(tidyverse)
library(gridExtra)
library(latex2exp)
library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
library(bayesplot)
theme_set(bayesplot::theme_default(base_family = "sans"))
source('monitornew.R')
source('monitorplot.R')
```

## Cauchy: A distribution with infinite mean and variance

The classic *split*-$\widehat{R}$ is based on calculating within and between
chain variances. If the marginal distribution of a chain is such that the
variance is not defined (i.e., infinite), the classic *split*-$\widehat{R}$ is
not well justified. In this section, we will use the Cauchy distribution as an
example of such a distribution.

The following Cauchy models are from Michael Betancourt's case study
[Fitting The Cauchy Distribution](https://betanalpha.github.io/assets/case_studies/fitting_the_cauchy.html)

### Nominal parameterization of Cauchy

The nominal Cauchy model with direct parameterization is as follows:

```{r}
writeLines(readLines("cauchy_nom.stan"))
```

#### Default Stan options

Run the nominal model:
```{r fit_nom, cache=TRUE, comment=NA, results='hide'}
fit_nom <- stan(file = 'cauchy_nom.stan', seed = 7878, refresh = 0)
```

Treedepth exceedence and Bayesian Fraction of Missing Information are
dynamic HMC specific diagnostics [@betancourt2017conceptual]. We get
warnings about a very large number of transitions after
warmup that exceeded the maximum treedepth, which is likely due to
very long tails of the Cauchy distribution. All chains have low 
estimated Bayesian fraction of missing information also indicating
slow mixing.

```{r}
mon <- monitornew(fit_nom)
print(mon)
which_min_eff <- which.min(mon[1:50, 'Bulk_Reff'])
```

Several Rhat < 1.01 and some Reff > 0.1 indicate that the results should not be 
trusted. The extended case study [rhat_reff_extra](rhat_reff_extra.html) has 
some results for longer chains as well.

We can further analyze potential problems using local relative efficiency and
rank plots. We specifically investigate x[`r which_min_eff`], which has the
smallest bulk relative efficiency `r round(min(mon[,'Bulk_Reff']), 2)`. 

We examine the relative efficiency in different parts of the posterior by
computing the relative efficiency of small interval probability estimates (see
Section [Relative efficiency of small interval probability
estimates](#small_interval_R_eff)). Each interval contains $1/k$ of the draws
(e.g., $5\%$ each if $k=20$). The small interval efficiency measures mixing of
an function which indicates when the values are inside or outside the specific
small interval. As detailed above, this gives us a local efficiency measure
which does not depend on the shape of the distribution.

```{r}
plot_local_reff(fit = fit_nom, par = which_min_eff, nalpha = 20)
```

We see that the efficiency of our posterior draws is worryingly low in the tails
(which is caused by slow mixing in long tails of Cauchy). Orange ticks
show iterations that exceeded the maximum treedepth.

An alternative way to examine the relative efficiency in different parts of the
posterior is to compute relative efficiencies for quantiles (see Section
[Relative efficiency of quantiles](#quantile_R_eff)). Each interval has a
specified proportion of draws, and the efficiency measures mixing of a function
which indicates when the values are smaller than or equal to the corresponding
quantile.

```{r, cache=FALSE}
plot_quantile_reff(fit = fit_nom, par = which_min_eff, nalpha = 40)
```

Similar as above, we see that the efficiency of our posterior draws is worryingly
low in the tails. Again, orange ticks show iterations that exceeded the maximum
treedepth.

We can further analyze potential problems using rank plots in which we clearly
see differences between chains.

```{r}
samp <- as.array(fit_nom)
xmin <- paste0("x[", which_min_eff, "]")
mcmc_hist_r_scale(samp[, , xmin])
```


### Alternative parameterization of Cauchy

Next we examine an alternative parameterization that considers the Cauchy
distribution as a scale mixture of Gaussian distributions. The model has two
parameters and the Cauchy distributed $x$'s can be computed from those. In
addition to improved sampling performance, the example illustrates that focusing
on diagnostics matters.

```{r}
writeLines(readLines("cauchy_alt_1.stan"))
```

Run the alternative model:

```{r fit_alt1, cache=TRUE, comment=NA, results='hide'}
fit_alt1 <- stan(file = 'cauchy_alt_1.stan', seed = 7878, refresh = 0)
```

There are no warnings, and the sampling is much faster.

```{r, cache=FALSE}
mon <- monitornew(fit_alt1)
print(mon)
which_min_eff <- which.min(mon[101:150, 'Bulk_Reff'])
```

All Rhat < 1.01 and Reff > 0.1 indicate the sampling worked much better with 
the alternative parameterization. The extended case study 
[rhat_reff_extra](rhat_reff_extra.html) has results for more alternative 
parameterizations.

We can further analyze potential problems using local relative efficiency and
rank plots. We take a detailed look at x[`r which_min_eff`], which has the 
smallest bulk relative efficiency of `r round(mon[which_min_eff, 'Bulk_Reff'], 2)`.

We examine the relative efficiency in different parts of the
posterior by computing the relative efficiency of small interval
probability estimates.

```{r}
plot_local_reff(fit = fit_alt1, par = which_min_eff + 100, nalpha = 20)
```

The relative efficiency is good in all parts of the posterior. Further, we 
examine the relative efficiency of different quantile estimates.

```{r}
plot_quantile_reff(fit = fit_alt1, par = which_min_eff + 100, nalpha = 40)
```

Rank plots also look rather similar across chains.

```{r}
samp <- as.array(fit_alt1)
xmin <- paste0("x[", which_min_eff, "]")
mcmc_hist_r_scale(samp[, , xmin])
```

In summary, the alterative parameterization produces results that look much
better than for the nominal parameterization. There are still some differences
in the tails, which we also identified via tail the relative efficiencies.


### Half-Cauchy with nominal parameterization

Half-Cauchy priors are common and, for example, in Stan usually set
using the nominal parameterization. However, when the constraint 
`<lower=0>` is used, Stan does the sampling automatically
in the unconstrained `log(x)` space, which changes the geometry
crucially.

```{r}
writeLines(readLines("half_cauchy_nom.stan"))
```

Run the half-Cauchy with nominal parameterization (and positive constraint):

```{r fit_half_nom, cache=TRUE, comment=NA, results='hide'}
fit_half_nom <- stan(file = 'half_cauchy_nom.stan', seed = 7878, refresh = 0)
```

There are no warnings, and the sampling is much faster than for the
Cauchy nominal model.

```{r, cache=FALSE}
mon <- monitornew(fit_half_nom)
print(mon)
```

All Rhat < 1.01 and Reff > 0.1 indicate good performance of the sampler. We see
that the Stan's automatic (and implicit) transformation of constraint parameters
can have a big effect on the sampling performance. More experimentes with
different parameterizations of the half-Cauchy distribution can be found in
[rhat_reff_extra.Rmd](rhat_reff_extra.html).


## Hierarchical model: Eight Schools

The Eight Schools data is a classic example for hierarchical models [see Section
5.5 in @BDA3], which despite the apparent simplicity nicely illustrates
the typical problems in inference for hierarchical models. The Stan
models below are from Michael Betancourt's case study on [Diagnosing
Biased Inference with
Divergences](http://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html).

### A Centered Eight Schools model

```{r, comment=NA}
writeLines(readLines("eight_schools_cp.stan"))
```

#### Centered Eight Schools model

We directly run the centered parameterization model with an increased
`adapt_delta` value to reduce the probability of getting divergent
transitions.
```{r fit_cp, cache=TRUE, comment=NA, results='hide'}
input_data <- read_rdump("eight_schools.data.R")
fit_cp <- stan(
  file = 'eight_schools_cp.stan', data = input_data,
  iter = 2000, chains = 4, seed = 483892929, refresh = 0,
  control = list(adapt_delta = 0.95)
)
```

Depite an increased `adapt_delta`, we still observe a lot of divergent
transitions, which in itself is already sufficient to not trust the results.
Still, we can use Rhat and Reff diagnostics to recognize problematic parts of
the posterior.

```{r, cache=FALSE}
mon <- monitornew(fit_cp)
print(mon)
```

See the extended case study [rhat_reff_extra](rhat_reff_extra.html) for 
results of longer chains.

We examine the relative efficiency in different parts of the posterior by
computing the relative efficiency of small interval estimates for
the between school standard deviation `tau`. These plots may either show
quantiles or parameter values at the vertical axis.

```{r}
plot_local_reff(fit = fit_cp, par = "tau", nalpha = 20)
```

```{r}
plot_local_reff(fit = fit_cp, par = "tau", nalpha = 20, rank = FALSE)
```

We see that the sampler has difficulties in exploring small `tau` values. As
the efficiency for estimating small `tau` values is practically zero,
we may assume that we may miss substantial amount of posterior mass
and get biased estimates. Red ticks, which show iterations with divergences,
have concentrated to small `tau` values, indicating problems exploring
even smaller values which is likely to cause more bias.

We examine also the relative efficiency of different quantile estimates. Again,
these plots may either show quantiles or parameter values at the vertical axis.

```{r}
plot_quantile_reff(fit = fit_cp, par = 2, nalpha = 40)
```

```{r}
plot_quantile_reff(fit = fit_cp, par = 2, nalpha = 40, rank = FALSE)
```

Most of the quantile estimates have worryingly low relative efficiency.

In lines with these findings, the rank plots of `tau` clearly show problems
in the mixing of the chains.

```{r}
samp_cp <- as.array(fit_cp)
mcmc_hist_r_scale(samp_cp[, , "tau"])
```


### Non-centered Eight Schools model

For hierarchical models, the non-centered parameterization often works better
than the centered one:

```{r, comment=NA}
writeLines(readLines("eight_schools_ncp.stan"))
```

#### Non-centered parameterization default MCMC options plus `adapt_delta=0.95`

For reasons of comparability, we als run the non-centered parameterization 
model with an increased `adapt_delta` value:

```{r fit_ncp2, cache=TRUE, comment=NA, results='hide'}
fit_ncp2 <- stan(
  file = 'eight_schools_ncp.stan', data = input_data,
  iter = 2000, chains = 4, seed = 483892929, refresh = 0,
  control = list(adapt_delta = 0.95)
)
```

We get zero divergences and no other warnings which is a first good sign.

```{r, cache=FALSE}
mon <- monitornew(fit_ncp2)
print(mon)
```

All Rhat < 1.01 and Rhat > 0.1 indicate a much better performance of the
non-centered parameterization.

We examine the relative efficiency in different parts of the
posterior by computing the relative efficiency of small interval
probability estimates for `tau`.

```{r}
plot_local_reff(fit = fit_ncp2, par = 2, nalpha = 20)
```

Small `tau` values are still more difficult to explore, but the relative
efficiency is in a good range. We may also check this with a finer resolution:

```{r}
plot_local_reff(fit = fit_ncp2, par = 2, nalpha = 50)
```

The relative efficiency of different quantile estimates looks good as well.

```{r}
plot_quantile_reff(fit = fit_ncp2, par = 2, nalpha = 40)
```

In line with these finding, the rank plots of `tau` show no substantial 
differences between chains.

```{r}
samp_ncp2 <- as.array(fit_ncp2)
mcmc_hist_r_scale(samp_ncp2[, , 2])
```

# References {.unnumbered}

<div id="refs"></div>

# Original Computing Environment {.unnumbered}

```{r, comment=NA}
makevars <- file.path(Sys.getenv("HOME"), ".R/Makevars")
if (file.exists(makevars)) {
  writeLines(readLines(makevars)) 
}
```

```{r, comment=NA}
devtools::session_info("rstan")
```

